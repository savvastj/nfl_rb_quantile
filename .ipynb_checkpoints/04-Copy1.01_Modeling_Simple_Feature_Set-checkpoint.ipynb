{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling based on simple set of features\n",
    "\n",
    "## The main point of this notebook is to set simple benchmarks using simple models and more complex models with a simple set of features\n",
    "\n",
    "## We will try to predict the ceiling and floor for each NFL prospect, so instead of running typical regression analysis (in which we would regress towards mean) we will develop quantile regression models to construct prediction intervals for each NFL player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, Imputer\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.regression.quantile_regression import QuantReg\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "\n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform as sp_uniform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up some global variables and plotting settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"white\", palette=\"colorblind\", font_scale=1.35, \n",
    "        rc={\"figure.figsize\":(12,9)})\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = 50\n",
    "\n",
    "# Global Variables \n",
    "RANDOM_STATE = 269\n",
    "N_JOBS = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in the data and set up features and target for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"processed_data/rb_train_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 332 entries, 0 to 331\n",
      "Data columns (total 40 columns):\n",
      "Player                332 non-null object\n",
      "Pos                   332 non-null object\n",
      "College               332 non-null object\n",
      "Ht                    332 non-null int64\n",
      "Wt                    332 non-null int64\n",
      "Forty                 332 non-null float64\n",
      "Vertical              257 non-null float64\n",
      "Bench                 243 non-null float64\n",
      "Broad Jump            261 non-null float64\n",
      "Cone                  196 non-null float64\n",
      "Shuttle               192 non-null float64\n",
      "Year                  332 non-null int64\n",
      "Pfr_ID                255 non-null object\n",
      "Sref_Cfb_ID           291 non-null object\n",
      "Team_Drafted          193 non-null object\n",
      "G                     291 non-null float64\n",
      "Rush_Att              291 non-null float64\n",
      "Rush_Yds              291 non-null float64\n",
      "Rush_TD               291 non-null float64\n",
      "Rec                   291 non-null float64\n",
      "Rec_Yds               291 non-null float64\n",
      "Rec_TD                291 non-null float64\n",
      "Plays                 291 non-null float64\n",
      "Scrim_Yds             291 non-null float64\n",
      "Scrim_TD              291 non-null float64\n",
      "Col_Yrs_Played        291 non-null float64\n",
      "Col_Last_Yr_Played    291 non-null float64\n",
      "Rush_Yds_per_Att      291 non-null float64\n",
      "Rec_Yds_per_Rec       291 non-null float64\n",
      "Scrim_Avg             291 non-null float64\n",
      "Rush_Att_per_G        291 non-null float64\n",
      "Rush_Yds_per_G        291 non-null float64\n",
      "Rush_TD_per_G         291 non-null float64\n",
      "Rec_per_G             291 non-null float64\n",
      "Rec_Yds_per_G         291 non-null float64\n",
      "Rec_TD_per_G          291 non-null float64\n",
      "Plays_per_G           291 non-null float64\n",
      "Scrim_Yds_per_G       291 non-null float64\n",
      "Scrim_TD_per_G        291 non-null float64\n",
      "AV                    332 non-null float64\n",
      "dtypes: float64(31), int64(3), object(6)\n",
      "memory usage: 103.8+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.replace([np.inf, -np.inf], np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select basic features for modeling\n",
    "### I will just use basic Rushing Features and Combine information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create yearly average rushing stats\n",
    "# rush_per_yr_cols = [\"Rush_Att_per_Yr\", \"Rush_Yds_per_Yr\", \"Rush_TD_per_Yr\"]\n",
    "# train_df[rush_per_yr_cols] = (train_df.loc[:, \"Rush_Att\":\"Rush_TD\"]\n",
    "#                                       .div(train_df.Col_Yrs_Played, axis=0)\n",
    "#                                       .fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"Rush_Att\",\n",
    "            \"Rush_Yds\",\n",
    "            \"Rush_TD\",\n",
    "            \"Rush_Yds_per_Att\",\n",
    "            \"Rush_Att_per_G\",\n",
    "            \"Rush_Yds_per_G\",\n",
    "            \"Rush_TD_per_G\",\n",
    "#             \"Rush_Att_per_Yr\",\n",
    "#             \"Rush_Yds_per_Yr\",\n",
    "#             \"Rush_TD_per_Yr\",\n",
    "            \"Ht\",\n",
    "            \"Wt\",\n",
    "            \"Forty\",\n",
    "            \"Vertical\",\n",
    "            \"Bench\",\n",
    "            \"Broad Jump\",\n",
    "            \"Cone\",\n",
    "            \"Shuttle\"]\n",
    "\n",
    "target = \"AV\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.loc[:, features]\n",
    "y = train_df.loc[:, target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct CV folds based on years (i.e. Leave One Season Out Cross-Validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get a dictionary with the season as the key and the associated\n",
    "# # row indices as the values\n",
    "year_grpby = train_df.groupby(\"Year\")\n",
    "year_grpby.grouper.label_info\n",
    "logo = LeaveOneGroupOut()\n",
    "cv = list(logo.split(X, y, year_grpby.grouper.label_info))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Benchmarks\n",
    "### Before setting up more complex models it\"s good to have some basic benchmark models to compare to.\n",
    "\n",
    "### Going forward we will evaluate models based on Quantile Loss (a.k.a. Pinball Loss)\n",
    "\n",
    "### Downside of RB will be represented by the 10% quantile (or percentile)\n",
    "### Upside will be the 90% quantile\n",
    "### And we will also try and predict the median (50%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile_loss(y_true, y_pred, quantile=0.95):\n",
    "    \"\"\"\n",
    "    Quantile Loss function to be used to score different qunatile regression \n",
    "    models. The code below is based on the logic from this blog post:\n",
    "    https://www.lokad.com/pinball-loss-function-definition\n",
    "    \"\"\"\n",
    "    loss = np.nanmean(\n",
    "            # if real value (y_true) >= prediction (y_pred)\n",
    "            # then calculate (real value - prediction) * quantile\n",
    "            ((y_true >= y_pred) * (y_true - y_pred) * quantile) +\n",
    "            # otherwise, if real value < prediction\n",
    "            # then caclulate (prediction - real value) * (1 - quantile)\n",
    "            (y_true < y_pred) * (y_pred - y_true) * (1 - quantile)\n",
    "    )\n",
    "\n",
    "    return loss\n",
    "\n",
    "# create scorers\n",
    "quant_90_scorer = make_scorer(quantile_loss, greater_is_better=False, \n",
    "                              quantile=.9)\n",
    "quant_50_scorer = make_scorer(quantile_loss, greater_is_better=False,\n",
    "                              quantile=0.5)\n",
    "quant_10_scorer = make_scorer(quantile_loss, greater_is_better=False,\n",
    "                              quantile=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 90th quantile benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean quantile loss: 3.150498868506221\n",
      "STDEV quantile loss: 0.5285265588088959\n"
     ]
    }
   ],
   "source": [
    "dummy_90_model = DummyRegressor(strategy=\"quantile\", quantile=.9)\n",
    "\n",
    "dummy_90_scores = cross_val_score(dummy_90_model, X, y, \n",
    "                                    cv=cv, n_jobs=N_JOBS, \n",
    "                                    scoring=quant_90_scorer)\n",
    "\n",
    "print(\"Mean quantile loss:\", abs(dummy_90_scores.mean()))\n",
    "print(\"STDEV quantile loss:\", dummy_90_scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Median (50th quantile) benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean quantile loss: 4.324091687179923\n",
      "STDEV quantile loss: 1.3574358908741526\n"
     ]
    }
   ],
   "source": [
    "dummy_50_model = DummyRegressor(strategy=\"median\")\n",
    "\n",
    "dummy_50_scores = cross_val_score(dummy_50_model, X, y, \n",
    "                                    cv=cv, n_jobs=N_JOBS, \n",
    "                                    scoring=quant_50_scorer)\n",
    "\n",
    "print(\"Mean quantile loss:\", abs(dummy_50_scores.mean()))\n",
    "print(\"STDEV quantile loss:\", dummy_50_scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10th quantile benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean quantile loss: 0.8889510554804673\n",
      "STDEV quantile loss: 0.2900529648228484\n"
     ]
    }
   ],
   "source": [
    "dummy_10_model = DummyRegressor(strategy=\"quantile\", quantile=.1)\n",
    "\n",
    "dummy_10_scores = cross_val_score(dummy_10_model, X, y, \n",
    "                                    cv=cv, n_jobs=N_JOBS, \n",
    "                                    scoring=quant_10_scorer)\n",
    "\n",
    "print(\"Mean quantile loss:\", abs(dummy_10_scores.mean()))\n",
    "print(\"STDEV quantile loss:\", dummy_10_scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Modeling Pipeline\n",
    "\n",
    "## The modeling pipeline will consists of 3 parts, a scaler (for linear methods), an imputer (to fill in the missing data) and the estimator (i.e. learning algorithm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([(\"imputer\", Imputer()),\n",
    "                 (\"scaler\", StandardScaler()),\n",
    "                 (\"estimator\", LGBMRegressor())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and Evaluate Models for 90th quantile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SMWrapper(BaseEstimator, RegressorMixin):\n",
    "    \"\"\" \n",
    "    A universal sklearn-style wrapper for statsmodels regressors based on code\n",
    "    from here:\n",
    "    https://stackoverflow.com/questions/41045752/using-statsmodel-estimations-with-scikit-learn-cross-validation-is-it-possible\n",
    "    \"\"\"\n",
    "    def __init__(self, model_class, fit_intercept=True, fit_kwargs={}):\n",
    "        self.model_class = model_class\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.fit_kwargs = fit_kwargs\n",
    "    def fit(self, X, y):\n",
    "        if self.fit_intercept:\n",
    "            X = sm.add_constant(X)\n",
    "        self.model_ = self.model_class(y, X)\n",
    "        self.results_ = self.model_.fit(**self.fit_kwargs)\n",
    "    def predict(self, X):\n",
    "        if self.fit_intercept:\n",
    "            X = sm.add_constant(X)\n",
    "        return self.results_.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search space for normal quantile rergression\n",
    "imputation_strategies =  [\"mean\", \"median\", \"most_frequent\"]\n",
    "qr_90_param_space = {\n",
    "    \"imputer__strategy\": imputation_strategies,\n",
    "    \"estimator\": [SMWrapper(QuantReg, fit_kwargs={\"q\":0.9})]\n",
    "}\n",
    "\n",
    "# # search space for LGBM quantile regression\n",
    "# lgbm_90_param_space = {\n",
    "#     \"imputer__strategy\": Categorical(imputation_strategies),\n",
    "#     \"scaler\": Categorical([None]),\n",
    "#     \"estimator\": Categorical([LGBMRegressor(random_state=RANDOM_STATE,\n",
    "#                                             alpha=0.9,\n",
    "#                                             objective=\"quantile\",\n",
    "#                                             n_estimtors=100)]),\n",
    "#     \"estimator__max_depth\": Integer(1, 25),\n",
    "#     \"estimator__learning_rate\" : Real(0.01, 0.3),\n",
    "#     \"estimator__num_leaves\": Integer(10, 500)\n",
    "# }\n",
    "\n",
    "# search space for LGBM quantile regression\n",
    "lgbm_90_param_space = {\n",
    "    \"imputer__strategy\": imputation_strategies,\n",
    "    \"scaler\": [None],\n",
    "    \"estimator\": [LGBMRegressor(random_state=RANDOM_STATE,\n",
    "                                alpha=0.5,\n",
    "                                objective=\"quantile\",\n",
    "                                n_estimtors=100)],\n",
    "    \"estimator__max_depth\": sp_randint(1, 25),\n",
    "    \"estimator__learning_rate\" : sp_uniform(0.01, 0.3),\n",
    "    \"estimator__num_leaves\": sp_randint(10, 500)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate QR 90\n",
    "#### Use gridsearch for normal quantile regression since we are only searching over imputation strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 12 folds for each of 3 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/savvas/miniconda3/envs/nfl_quant/lib/python3.6/site-packages/statsmodels/regression/quantile_regression.py:193: IterationLimitWarning: Maximum number of iterations (1000) reached.\n",
      "  \") reached.\", IterationLimitWarning)\n",
      "/home/savvas/miniconda3/envs/nfl_quant/lib/python3.6/site-packages/statsmodels/regression/quantile_regression.py:193: IterationLimitWarning: Maximum number of iterations (1000) reached.\n",
      "  \") reached.\", IterationLimitWarning)\n",
      "[Parallel(n_jobs=6)]: Done  36 out of  36 | elapsed:    3.9s finished\n",
      "/home/savvas/miniconda3/envs/nfl_quant/lib/python3.6/site-packages/statsmodels/regression/quantile_regression.py:193: IterationLimitWarning: Maximum number of iterations (1000) reached.\n",
      "  \") reached.\", IterationLimitWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=[(array([ 34,  35, ..., 330, 331]), array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33])), (array([  0,   1, ..., 330, 331]), array([34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50... 314, 315, 316, 317, 318, 319, 320, 321, 322, 323,\n",
       "       324, 325, 326, 327, 328, 329, 330, 331]))],\n",
       "       error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('estimator', LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "       learning_rate=0.1, max_depth=-1, m....0, reg_lambda=0.0, silent=True, subsample=1.0,\n",
       "       subsample_for_bin=200000, subsample_freq=1))]),\n",
       "       fit_params=None, iid=True, n_jobs=6,\n",
       "       param_grid={'imputer__strategy': ['mean', 'median', 'most_frequent'], 'estimator': [SMWrapper(fit_intercept=True, fit_kwargs={'q': 0.9},\n",
       "     model_class=<class 'statsmodels.regression.quantile_regression.QuantReg'>)]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "       scoring=make_scorer(quantile_loss, greater_is_better=False, quantile=0.9),\n",
       "       verbose=1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qr_90_grid_search = GridSearchCV(pipe, n_jobs=N_JOBS, param_grid=qr_90_param_space,\n",
    "                                 cv=cv, scoring=quant_90_scorer, verbose=1,\n",
    "                                 return_train_score=False)\n",
    "qr_90_grid_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('estimator', SMWrapper(fit_intercept=True, fit_kwargs={'q': 0.9},\n",
       "     model_class=<class 'statsmodels.regression.quantile_regression.QuantReg'>))])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qr_90_grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "qr_90_cv_results = pd.DataFrame(qr_90_grid_search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>param_imputer__strategy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.806068</td>\n",
       "      <td>0.723447</td>\n",
       "      <td>median</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.880352</td>\n",
       "      <td>0.697807</td>\n",
       "      <td>most_frequent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2.881948</td>\n",
       "      <td>0.719434</td>\n",
       "      <td>mean</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_test_score  std_test_score param_imputer__strategy\n",
       "1        -2.806068        0.723447                  median\n",
       "2        -2.880352        0.697807           most_frequent\n",
       "0        -2.881948        0.719434                    mean"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(qr_90_cv_results[[\"mean_test_score\", \"std_test_score\", \"param_imputer__strategy\"]]\n",
    "     .sort_values(\"mean_test_score\", ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate LGBM 90\n",
    "\n",
    "### Here we will use Bayesian Optimization to tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgbm_90_search = BayesSearchCV(pipe, \n",
    "#                                      lgbm_90_param_space, \n",
    "#                                      cv=cv,\n",
    "#                                      n_jobs=N_JOBS, \n",
    "#                                      verbose=1, \n",
    "#                                      error_score=-9999, \n",
    "#                                      scoring=quant_90_scorer, \n",
    "#                                      random_state=RANDOM_STATE,\n",
    "#                                      return_train_score=False, \n",
    "#                                      n_iter=18)\n",
    "\n",
    "lgbm_90_search = RandomizedSearchCV(pipe, \n",
    "                                    lgbm_90_param_space, \n",
    "                                    cv=cv,\n",
    "                                    n_jobs=N_JOBS, \n",
    "                                    verbose=2, \n",
    "                                    error_score=-9999, \n",
    "                                    scoring=quant_90_scorer, \n",
    "                                    random_state=RANDOM_STATE,\n",
    "                                    return_train_score=False, \n",
    "                                    n_iter=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 12 folds for each of 2 candidates, totalling 24 fits\n",
      "[CV] estimator=LGBMRegressor(alpha=0.5, boosting_type='gbdt', class_weight=None,\n",
      "       colsample_bytree=1.0, learning_rate=0.1, max_depth=-1,\n",
      "       min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "       n_estimators=100, n_estimtors=100, n_jobs=-1, num_leaves=31,\n",
      "       objective='quantile', random_state=269, reg_alpha=0.0,\n",
      "       reg_lambda=0.0, silent=True, subsample=1.0,\n",
      "       subsample_for_bin=200000, subsample_freq=1), estimator__learning_rate=0.08749548119134132, estimator__max_depth=9, estimator__num_leaves=495, imputer__strategy=mean, scaler=None \n",
      "[CV] estimator=LGBMRegressor(alpha=0.5, boosting_type='gbdt', class_weight=None,\n",
      "       colsample_bytree=1.0, learning_rate=0.1, max_depth=-1,\n",
      "       min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "       n_estimators=100, n_estimtors=100, n_jobs=-1, num_leaves=31,\n",
      "       objective='quantile', random_state=269, reg_alpha=0.0,\n",
      "       reg_lambda=0.0, silent=True, subsample=1.0,\n",
      "       subsample_for_bin=200000, subsample_freq=1), estimator__learning_rate=0.08749548119134132, estimator__max_depth=9, estimator__num_leaves=495, imputer__strategy=mean, scaler=None \n",
      "[CV] estimator=LGBMRegressor(alpha=0.5, boosting_type='gbdt', class_weight=None,\n",
      "       colsample_bytree=1.0, learning_rate=0.1, max_depth=-1,\n",
      "       min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "       n_estimators=100, n_estimtors=100, n_jobs=-1, num_leaves=31,\n",
      "       objective='quantile', random_state=269, reg_alpha=0.0,\n",
      "       reg_lambda=0.0, silent=True, subsample=1.0,\n",
      "       subsample_for_bin=200000, subsample_freq=1), estimator__learning_rate=0.08749548119134132, estimator__max_depth=9, estimator__num_leaves=495, imputer__strategy=mean, scaler=None [CV] estimator=LGBMRegressor(alpha=0.5, boosting_type='gbdt', class_weight=None,\n",
      "       colsample_bytree=1.0, learning_rate=0.1, max_depth=-1,\n",
      "       min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "       n_estimators=100, n_estimtors=100, n_jobs=-1, num_leaves=31,\n",
      "       objective='quantile', random_state=269, reg_alpha=0.0,\n",
      "       reg_lambda=0.0, silent=True, subsample=1.0,\n",
      "       subsample_for_bin=200000, subsample_freq=1), estimator__learning_rate=0.08749548119134132, estimator__max_depth=9, estimator__num_leaves=495, imputer__strategy=mean, scaler=None \n",
      "[CV] estimator=LGBMRegressor(alpha=0.5, boosting_type='gbdt', class_weight=None,\n",
      "       colsample_bytree=1.0, learning_rate=0.1, max_depth=-1,\n",
      "       min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "       n_estimators=100, n_estimtors=100, n_jobs=-1, num_leaves=31,\n",
      "       objective='quantile', random_state=269, reg_alpha=0.0,\n",
      "       reg_lambda=0.0, silent=True, subsample=1.0,\n",
      "       subsample_for_bin=200000, subsample_freq=1), estimator__learning_rate=0.08749548119134132, estimator__max_depth=9, estimator__num_leaves=495, imputer__strategy=mean, scaler=None \n",
      "\n",
      "[CV] estimator=LGBMRegressor(alpha=0.5, boosting_type='gbdt', class_weight=None,\n",
      "       colsample_bytree=1.0, learning_rate=0.1, max_depth=-1,\n",
      "       min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "       n_estimators=100, n_estimtors=100, n_jobs=-1, num_leaves=31,\n",
      "       objective='quantile', random_state=269, reg_alpha=0.0,\n",
      "       reg_lambda=0.0, silent=True, subsample=1.0,\n",
      "       subsample_for_bin=200000, subsample_freq=1), estimator__learning_rate=0.08749548119134132, estimator__max_depth=9, estimator__num_leaves=495, imputer__strategy=mean, scaler=None \n",
      "[CV]  estimator=LGBMRegressor(alpha=0.5, boosting_type='gbdt', class_weight=None,\n",
      "       colsample_bytree=1.0, learning_rate=0.1, max_depth=-1,\n",
      "       min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "       n_estimators=100, n_estimtors=100, n_jobs=-1, num_leaves=31,\n",
      "       objective='quantile', random_state=269, reg_alpha=0.0,\n",
      "       reg_lambda=0.0, silent=True, subsample=1.0,\n",
      "       subsample_for_bin=200000, subsample_freq=1), estimator__learning_rate=0.08749548119134132, estimator__max_depth=9, estimator__num_leaves=495, imputer__strategy=mean, scaler=None, total=  33.8s\n",
      "[CV] estimator=LGBMRegressor(alpha=0.5, boosting_type='gbdt', class_weight=None,\n",
      "       colsample_bytree=1.0, learning_rate=0.1, max_depth=-1,\n",
      "       min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "       n_estimators=100, n_estimtors=100, n_jobs=-1, num_leaves=31,\n",
      "       objective='quantile', random_state=269, reg_alpha=0.0,\n",
      "       reg_lambda=0.0, silent=True, subsample=1.0,\n",
      "       subsample_for_bin=200000, subsample_freq=1), estimator__learning_rate=0.08749548119134132, estimator__max_depth=9, estimator__num_leaves=495, imputer__strategy=mean, scaler=None \n",
      "[CV]  estimator=LGBMRegressor(alpha=0.5, boosting_type='gbdt', class_weight=None,\n",
      "       colsample_bytree=1.0, learning_rate=0.1, max_depth=-1,\n",
      "       min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "       n_estimators=100, n_estimtors=100, n_jobs=-1, num_leaves=31,\n",
      "       objective='quantile', random_state=269, reg_alpha=0.0,\n",
      "       reg_lambda=0.0, silent=True, subsample=1.0,\n",
      "       subsample_for_bin=200000, subsample_freq=1), estimator__learning_rate=0.08749548119134132, estimator__max_depth=9, estimator__num_leaves=495, imputer__strategy=mean, scaler=None, total=  35.4s\n",
      "[CV] estimator=LGBMRegressor(alpha=0.5, boosting_type='gbdt', class_weight=None,\n",
      "       colsample_bytree=1.0, learning_rate=0.1, max_depth=-1,\n",
      "       min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "       n_estimators=100, n_estimtors=100, n_jobs=-1, num_leaves=31,\n",
      "       objective='quantile', random_state=269, reg_alpha=0.0,\n",
      "       reg_lambda=0.0, silent=True, subsample=1.0,\n",
      "       subsample_for_bin=200000, subsample_freq=1), estimator__learning_rate=0.08749548119134132, estimator__max_depth=9, estimator__num_leaves=495, imputer__strategy=mean, scaler=None \n",
      "[CV]  estimator=LGBMRegressor(alpha=0.5, boosting_type='gbdt', class_weight=None,\n",
      "       colsample_bytree=1.0, learning_rate=0.1, max_depth=-1,\n",
      "       min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "       n_estimators=100, n_estimtors=100, n_jobs=-1, num_leaves=31,\n",
      "       objective='quantile', random_state=269, reg_alpha=0.0,\n",
      "       reg_lambda=0.0, silent=True, subsample=1.0,\n",
      "       subsample_for_bin=200000, subsample_freq=1), estimator__learning_rate=0.08749548119134132, estimator__max_depth=9, estimator__num_leaves=495, imputer__strategy=mean, scaler=None, total=  36.2s\n",
      "[CV] estimator=LGBMRegressor(alpha=0.5, boosting_type='gbdt', class_weight=None,\n",
      "       colsample_bytree=1.0, learning_rate=0.1, max_depth=-1,\n",
      "       min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "       n_estimators=100, n_estimtors=100, n_jobs=-1, num_leaves=31,\n",
      "       objective='quantile', random_state=269, reg_alpha=0.0,\n",
      "       reg_lambda=0.0, silent=True, subsample=1.0,\n",
      "       subsample_for_bin=200000, subsample_freq=1), estimator__learning_rate=0.08749548119134132, estimator__max_depth=9, estimator__num_leaves=495, imputer__strategy=mean, scaler=None \n",
      "[CV]  estimator=LGBMRegressor(alpha=0.5, boosting_type='gbdt', class_weight=None,\n",
      "       colsample_bytree=1.0, learning_rate=0.1, max_depth=-1,\n",
      "       min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "       n_estimators=100, n_estimtors=100, n_jobs=-1, num_leaves=31,\n",
      "       objective='quantile', random_state=269, reg_alpha=0.0,\n",
      "       reg_lambda=0.0, silent=True, subsample=1.0,\n",
      "       subsample_for_bin=200000, subsample_freq=1), estimator__learning_rate=0.08749548119134132, estimator__max_depth=9, estimator__num_leaves=495, imputer__strategy=mean, scaler=None, total=  36.5s\n",
      "[CV] estimator=LGBMRegressor(alpha=0.5, boosting_type='gbdt', class_weight=None,\n",
      "       colsample_bytree=1.0, learning_rate=0.1, max_depth=-1,\n",
      "       min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "       n_estimators=100, n_estimtors=100, n_jobs=-1, num_leaves=31,\n",
      "       objective='quantile', random_state=269, reg_alpha=0.0,\n",
      "       reg_lambda=0.0, silent=True, subsample=1.0,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       subsample_for_bin=200000, subsample_freq=1), estimator__learning_rate=0.08749548119134132, estimator__max_depth=9, estimator__num_leaves=495, imputer__strategy=mean, scaler=None \n",
      "[CV]  estimator=LGBMRegressor(alpha=0.5, boosting_type='gbdt', class_weight=None,\n",
      "       colsample_bytree=1.0, learning_rate=0.1, max_depth=-1,\n",
      "       min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "       n_estimators=100, n_estimtors=100, n_jobs=-1, num_leaves=31,\n",
      "       objective='quantile', random_state=269, reg_alpha=0.0,\n",
      "       reg_lambda=0.0, silent=True, subsample=1.0,\n",
      "       subsample_for_bin=200000, subsample_freq=1), estimator__learning_rate=0.08749548119134132, estimator__max_depth=9, estimator__num_leaves=495, imputer__strategy=mean, scaler=None, total=  36.8s\n",
      "[CV] estimator=LGBMRegressor(alpha=0.5, boosting_type='gbdt', class_weight=None,\n",
      "       colsample_bytree=1.0, learning_rate=0.1, max_depth=-1,\n",
      "       min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "       n_estimators=100, n_estimtors=100, n_jobs=-1, num_leaves=31,\n",
      "       objective='quantile', random_state=269, reg_alpha=0.0,\n",
      "       reg_lambda=0.0, silent=True, subsample=1.0,\n",
      "       subsample_for_bin=200000, subsample_freq=1), estimator__learning_rate=0.08749548119134132, estimator__max_depth=9, estimator__num_leaves=495, imputer__strategy=mean, scaler=None \n",
      "[CV]  estimator=LGBMRegressor(alpha=0.5, boosting_type='gbdt', class_weight=None,\n",
      "       colsample_bytree=1.0, learning_rate=0.1, max_depth=-1,\n",
      "       min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "       n_estimators=100, n_estimtors=100, n_jobs=-1, num_leaves=31,\n",
      "       objective='quantile', random_state=269, reg_alpha=0.0,\n",
      "       reg_lambda=0.0, silent=True, subsample=1.0,\n",
      "       subsample_for_bin=200000, subsample_freq=1), estimator__learning_rate=0.08749548119134132, estimator__max_depth=9, estimator__num_leaves=495, imputer__strategy=mean, scaler=None, total=  36.9s\n",
      "[CV] estimator=LGBMRegressor(alpha=0.5, boosting_type='gbdt', class_weight=None,\n",
      "       colsample_bytree=1.0, learning_rate=0.1, max_depth=-1,\n",
      "       min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "       n_estimators=100, n_estimtors=100, n_jobs=-1, num_leaves=31,\n",
      "       objective='quantile', random_state=269, reg_alpha=0.0,\n",
      "       reg_lambda=0.0, silent=True, subsample=1.0,\n",
      "       subsample_for_bin=200000, subsample_freq=1), estimator__learning_rate=0.08749548119134132, estimator__max_depth=9, estimator__num_leaves=495, imputer__strategy=mean, scaler=None \n",
      "[CV]  estimator=LGBMRegressor(alpha=0.5, boosting_type='gbdt', class_weight=None,\n",
      "       colsample_bytree=1.0, learning_rate=0.1, max_depth=-1,\n",
      "       min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "       n_estimators=100, n_estimtors=100, n_jobs=-1, num_leaves=31,\n",
      "       objective='quantile', random_state=269, reg_alpha=0.0,\n",
      "       reg_lambda=0.0, silent=True, subsample=1.0,\n",
      "       subsample_for_bin=200000, subsample_freq=1), estimator__learning_rate=0.08749548119134132, estimator__max_depth=9, estimator__num_leaves=495, imputer__strategy=mean, scaler=None, total=  43.6s\n",
      "[CV] estimator=LGBMRegressor(alpha=0.5, boosting_type='gbdt', class_weight=None,\n",
      "       colsample_bytree=1.0, learning_rate=0.1, max_depth=-1,\n",
      "       min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "       n_estimators=100, n_estimtors=100, n_jobs=-1, num_leaves=31,\n",
      "       objective='quantile', random_state=269, reg_alpha=0.0,\n",
      "       reg_lambda=0.0, silent=True, subsample=1.0,\n",
      "       subsample_for_bin=200000, subsample_freq=1), estimator__learning_rate=0.12387064863290859, estimator__max_depth=16, estimator__num_leaves=430, imputer__strategy=most_frequent, scaler=None \n",
      "[CV]  estimator=LGBMRegressor(alpha=0.5, boosting_type='gbdt', class_weight=None,\n",
      "       colsample_bytree=1.0, learning_rate=0.1, max_depth=-1,\n",
      "       min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "       n_estimators=100, n_estimtors=100, n_jobs=-1, num_leaves=31,\n",
      "       objective='quantile', random_state=269, reg_alpha=0.0,\n",
      "       reg_lambda=0.0, silent=True, subsample=1.0,\n",
      "       subsample_for_bin=200000, subsample_freq=1), estimator__learning_rate=0.08749548119134132, estimator__max_depth=9, estimator__num_leaves=495, imputer__strategy=mean, scaler=None, total=  42.7s\n",
      "[CV] estimator=LGBMRegressor(alpha=0.5, boosting_type='gbdt', class_weight=None,\n",
      "       colsample_bytree=1.0, learning_rate=0.1, max_depth=-1,\n",
      "       min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "       n_estimators=100, n_estimtors=100, n_jobs=-1, num_leaves=31,\n",
      "       objective='quantile', random_state=269, reg_alpha=0.0,\n",
      "       reg_lambda=0.0, silent=True, subsample=1.0,\n",
      "       subsample_for_bin=200000, subsample_freq=1), estimator__learning_rate=0.12387064863290859, estimator__max_depth=16, estimator__num_leaves=430, imputer__strategy=most_frequent, scaler=None \n",
      "[CV]  estimator=LGBMRegressor(alpha=0.5, boosting_type='gbdt', class_weight=None,\n",
      "       colsample_bytree=1.0, learning_rate=0.1, max_depth=-1,\n",
      "       min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "       n_estimators=100, n_estimtors=100, n_jobs=-1, num_leaves=31,\n",
      "       objective='quantile', random_state=269, reg_alpha=0.0,\n",
      "       reg_lambda=0.0, silent=True, subsample=1.0,\n",
      "       subsample_for_bin=200000, subsample_freq=1), estimator__learning_rate=0.08749548119134132, estimator__max_depth=9, estimator__num_leaves=495, imputer__strategy=mean, scaler=None, total=  42.1s\n",
      "[CV] estimator=LGBMRegressor(alpha=0.5, boosting_type='gbdt', class_weight=None,\n",
      "       colsample_bytree=1.0, learning_rate=0.1, max_depth=-1,\n",
      "       min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "       n_estimators=100, n_estimtors=100, n_jobs=-1, num_leaves=31,\n",
      "       objective='quantile', random_state=269, reg_alpha=0.0,\n",
      "       reg_lambda=0.0, silent=True, subsample=1.0,\n",
      "       subsample_for_bin=200000, subsample_freq=1), estimator__learning_rate=0.12387064863290859, estimator__max_depth=16, estimator__num_leaves=430, imputer__strategy=most_frequent, scaler=None \n",
      "[CV]  estimator=LGBMRegressor(alpha=0.5, boosting_type='gbdt', class_weight=None,\n",
      "       colsample_bytree=1.0, learning_rate=0.1, max_depth=-1,\n",
      "       min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "       n_estimators=100, n_estimtors=100, n_jobs=-1, num_leaves=31,\n",
      "       objective='quantile', random_state=269, reg_alpha=0.0,\n",
      "       reg_lambda=0.0, silent=True, subsample=1.0,\n",
      "       subsample_for_bin=200000, subsample_freq=1), estimator__learning_rate=0.08749548119134132, estimator__max_depth=9, estimator__num_leaves=495, imputer__strategy=mean, scaler=None, total=  44.0s\n",
      "[CV] estimator=LGBMRegressor(alpha=0.5, boosting_type='gbdt', class_weight=None,\n",
      "       colsample_bytree=1.0, learning_rate=0.1, max_depth=-1,\n",
      "       min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "       n_estimators=100, n_estimtors=100, n_jobs=-1, num_leaves=31,\n",
      "       objective='quantile', random_state=269, reg_alpha=0.0,\n",
      "       reg_lambda=0.0, silent=True, subsample=1.0,\n",
      "       subsample_for_bin=200000, subsample_freq=1), estimator__learning_rate=0.12387064863290859, estimator__max_depth=16, estimator__num_leaves=430, imputer__strategy=most_frequent, scaler=None \n",
      "[CV]  estimator=LGBMRegressor(alpha=0.5, boosting_type='gbdt', class_weight=None,\n",
      "       colsample_bytree=1.0, learning_rate=0.1, max_depth=-1,\n",
      "       min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "       n_estimators=100, n_estimtors=100, n_jobs=-1, num_leaves=31,\n",
      "       objective='quantile', random_state=269, reg_alpha=0.0,\n",
      "       reg_lambda=0.0, silent=True, subsample=1.0,\n",
      "       subsample_for_bin=200000, subsample_freq=1), estimator__learning_rate=0.08749548119134132, estimator__max_depth=9, estimator__num_leaves=495, imputer__strategy=mean, scaler=None, total=  44.0s\n",
      "[CV] estimator=LGBMRegressor(alpha=0.5, boosting_type='gbdt', class_weight=None,\n",
      "       colsample_bytree=1.0, learning_rate=0.1, max_depth=-1,\n",
      "       min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "       n_estimators=100, n_estimtors=100, n_jobs=-1, num_leaves=31,\n",
      "       objective='quantile', random_state=269, reg_alpha=0.0,\n",
      "       reg_lambda=0.0, silent=True, subsample=1.0,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       subsample_for_bin=200000, subsample_freq=1), estimator__learning_rate=0.12387064863290859, estimator__max_depth=16, estimator__num_leaves=430, imputer__strategy=most_frequent, scaler=None \n",
      "[CV]  estimator=LGBMRegressor(alpha=0.5, boosting_type='gbdt', class_weight=None,\n",
      "       colsample_bytree=1.0, learning_rate=0.1, max_depth=-1,\n",
      "       min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "       n_estimators=100, n_estimtors=100, n_jobs=-1, num_leaves=31,\n",
      "       objective='quantile', random_state=269, reg_alpha=0.0,\n",
      "       reg_lambda=0.0, silent=True, subsample=1.0,\n",
      "       subsample_for_bin=200000, subsample_freq=1), estimator__learning_rate=0.08749548119134132, estimator__max_depth=9, estimator__num_leaves=495, imputer__strategy=mean, scaler=None, total=  45.5s\n",
      "[CV] estimator=LGBMRegressor(alpha=0.5, boosting_type='gbdt', class_weight=None,\n",
      "       colsample_bytree=1.0, learning_rate=0.1, max_depth=-1,\n",
      "       min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "       n_estimators=100, n_estimtors=100, n_jobs=-1, num_leaves=31,\n",
      "       objective='quantile', random_state=269, reg_alpha=0.0,\n",
      "       reg_lambda=0.0, silent=True, subsample=1.0,\n",
      "       subsample_for_bin=200000, subsample_freq=1), estimator__learning_rate=0.12387064863290859, estimator__max_depth=16, estimator__num_leaves=430, imputer__strategy=most_frequent, scaler=None \n",
      "[CV]  estimator=LGBMRegressor(alpha=0.5, boosting_type='gbdt', class_weight=None,\n",
      "       colsample_bytree=1.0, learning_rate=0.1, max_depth=-1,\n",
      "       min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "       n_estimators=100, n_estimtors=100, n_jobs=-1, num_leaves=31,\n",
      "       objective='quantile', random_state=269, reg_alpha=0.0,\n",
      "       reg_lambda=0.0, silent=True, subsample=1.0,\n",
      "       subsample_for_bin=200000, subsample_freq=1), estimator__learning_rate=0.12387064863290859, estimator__max_depth=16, estimator__num_leaves=430, imputer__strategy=most_frequent, scaler=None, total=  50.3s\n",
      "[CV] estimator=LGBMRegressor(alpha=0.5, boosting_type='gbdt', class_weight=None,\n",
      "       colsample_bytree=1.0, learning_rate=0.1, max_depth=-1,\n",
      "       min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "       n_estimators=100, n_estimtors=100, n_jobs=-1, num_leaves=31,\n",
      "       objective='quantile', random_state=269, reg_alpha=0.0,\n",
      "       reg_lambda=0.0, silent=True, subsample=1.0,\n",
      "       subsample_for_bin=200000, subsample_freq=1), estimator__learning_rate=0.12387064863290859, estimator__max_depth=16, estimator__num_leaves=430, imputer__strategy=most_frequent, scaler=None \n",
      "[CV]  estimator=LGBMRegressor(alpha=0.5, boosting_type='gbdt', class_weight=None,\n",
      "       colsample_bytree=1.0, learning_rate=0.1, max_depth=-1,\n",
      "       min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "       n_estimators=100, n_estimtors=100, n_jobs=-1, num_leaves=31,\n",
      "       objective='quantile', random_state=269, reg_alpha=0.0,\n",
      "       reg_lambda=0.0, silent=True, subsample=1.0,\n",
      "       subsample_for_bin=200000, subsample_freq=1), estimator__learning_rate=0.12387064863290859, estimator__max_depth=16, estimator__num_leaves=430, imputer__strategy=most_frequent, scaler=None, total=  49.3s\n",
      "[CV] estimator=LGBMRegressor(alpha=0.5, boosting_type='gbdt', class_weight=None,\n",
      "       colsample_bytree=1.0, learning_rate=0.1, max_depth=-1,\n",
      "       min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "       n_estimators=100, n_estimtors=100, n_jobs=-1, num_leaves=31,\n",
      "       objective='quantile', random_state=269, reg_alpha=0.0,\n",
      "       reg_lambda=0.0, silent=True, subsample=1.0,\n",
      "       subsample_for_bin=200000, subsample_freq=1), estimator__learning_rate=0.12387064863290859, estimator__max_depth=16, estimator__num_leaves=430, imputer__strategy=most_frequent, scaler=None \n",
      "[CV]  estimator=LGBMRegressor(alpha=0.5, boosting_type='gbdt', class_weight=None,\n",
      "       colsample_bytree=1.0, learning_rate=0.1, max_depth=-1,\n",
      "       min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "       n_estimators=100, n_estimtors=100, n_jobs=-1, num_leaves=31,\n",
      "       objective='quantile', random_state=269, reg_alpha=0.0,\n",
      "       reg_lambda=0.0, silent=True, subsample=1.0,\n",
      "       subsample_for_bin=200000, subsample_freq=1), estimator__learning_rate=0.12387064863290859, estimator__max_depth=16, estimator__num_leaves=430, imputer__strategy=most_frequent, scaler=None, total=  50.8s\n",
      "[CV] estimator=LGBMRegressor(alpha=0.5, boosting_type='gbdt', class_weight=None,\n",
      "       colsample_bytree=1.0, learning_rate=0.1, max_depth=-1,\n",
      "       min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "       n_estimators=100, n_estimtors=100, n_jobs=-1, num_leaves=31,\n",
      "       objective='quantile', random_state=269, reg_alpha=0.0,\n",
      "       reg_lambda=0.0, silent=True, subsample=1.0,\n",
      "       subsample_for_bin=200000, subsample_freq=1), estimator__learning_rate=0.12387064863290859, estimator__max_depth=16, estimator__num_leaves=430, imputer__strategy=most_frequent, scaler=None \n",
      "[CV]  estimator=LGBMRegressor(alpha=0.5, boosting_type='gbdt', class_weight=None,\n",
      "       colsample_bytree=1.0, learning_rate=0.1, max_depth=-1,\n",
      "       min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "       n_estimators=100, n_estimtors=100, n_jobs=-1, num_leaves=31,\n",
      "       objective='quantile', random_state=269, reg_alpha=0.0,\n",
      "       reg_lambda=0.0, silent=True, subsample=1.0,\n",
      "       subsample_for_bin=200000, subsample_freq=1), estimator__learning_rate=0.12387064863290859, estimator__max_depth=16, estimator__num_leaves=430, imputer__strategy=most_frequent, scaler=None, total=  50.2s\n",
      "[CV] estimator=LGBMRegressor(alpha=0.5, boosting_type='gbdt', class_weight=None,\n",
      "       colsample_bytree=1.0, learning_rate=0.1, max_depth=-1,\n",
      "       min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "       n_estimators=100, n_estimtors=100, n_jobs=-1, num_leaves=31,\n",
      "       objective='quantile', random_state=269, reg_alpha=0.0,\n",
      "       reg_lambda=0.0, silent=True, subsample=1.0,\n",
      "       subsample_for_bin=200000, subsample_freq=1), estimator__learning_rate=0.12387064863290859, estimator__max_depth=16, estimator__num_leaves=430, imputer__strategy=most_frequent, scaler=None \n",
      "[CV]  estimator=LGBMRegressor(alpha=0.5, boosting_type='gbdt', class_weight=None,\n",
      "       colsample_bytree=1.0, learning_rate=0.1, max_depth=-1,\n",
      "       min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "       n_estimators=100, n_estimtors=100, n_jobs=-1, num_leaves=31,\n",
      "       objective='quantile', random_state=269, reg_alpha=0.0,\n",
      "       reg_lambda=0.0, silent=True, subsample=1.0,\n",
      "       subsample_for_bin=200000, subsample_freq=1), estimator__learning_rate=0.12387064863290859, estimator__max_depth=16, estimator__num_leaves=430, imputer__strategy=most_frequent, scaler=None, total=  50.4s\n",
      "[CV] estimator=LGBMRegressor(alpha=0.5, boosting_type='gbdt', class_weight=None,\n",
      "       colsample_bytree=1.0, learning_rate=0.1, max_depth=-1,\n",
      "       min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "       n_estimators=100, n_estimtors=100, n_jobs=-1, num_leaves=31,\n",
      "       objective='quantile', random_state=269, reg_alpha=0.0,\n",
      "       reg_lambda=0.0, silent=True, subsample=1.0,\n",
      "       subsample_for_bin=200000, subsample_freq=1), estimator__learning_rate=0.12387064863290859, estimator__max_depth=16, estimator__num_leaves=430, imputer__strategy=most_frequent, scaler=None \n",
      "[CV]  estimator=LGBMRegressor(alpha=0.5, boosting_type='gbdt', class_weight=None,\n",
      "       colsample_bytree=1.0, learning_rate=0.1, max_depth=-1,\n",
      "       min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "       n_estimators=100, n_estimtors=100, n_jobs=-1, num_leaves=31,\n",
      "       objective='quantile', random_state=269, reg_alpha=0.0,\n",
      "       reg_lambda=0.0, silent=True, subsample=1.0,\n",
      "       subsample_for_bin=200000, subsample_freq=1), estimator__learning_rate=0.12387064863290859, estimator__max_depth=16, estimator__num_leaves=430, imputer__strategy=most_frequent, scaler=None, total=  49.2s\n",
      "[CV] estimator=LGBMRegressor(alpha=0.5, boosting_type='gbdt', class_weight=None,\n",
      "       colsample_bytree=1.0, learning_rate=0.1, max_depth=-1,\n",
      "       min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "       n_estimators=100, n_estimtors=100, n_jobs=-1, num_leaves=31,\n",
      "       objective='quantile', random_state=269, reg_alpha=0.0,\n",
      "       reg_lambda=0.0, silent=True, subsample=1.0,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       subsample_for_bin=200000, subsample_freq=1), estimator__learning_rate=0.12387064863290859, estimator__max_depth=16, estimator__num_leaves=430, imputer__strategy=most_frequent, scaler=None \n",
      "[CV]  estimator=LGBMRegressor(alpha=0.5, boosting_type='gbdt', class_weight=None,\n",
      "       colsample_bytree=1.0, learning_rate=0.1, max_depth=-1,\n",
      "       min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "       n_estimators=100, n_estimtors=100, n_jobs=-1, num_leaves=31,\n",
      "       objective='quantile', random_state=269, reg_alpha=0.0,\n",
      "       reg_lambda=0.0, silent=True, subsample=1.0,\n",
      "       subsample_for_bin=200000, subsample_freq=1), estimator__learning_rate=0.12387064863290859, estimator__max_depth=16, estimator__num_leaves=430, imputer__strategy=most_frequent, scaler=None, total=  52.3s\n",
      "[CV]  estimator=LGBMRegressor(alpha=0.5, boosting_type='gbdt', class_weight=None,\n",
      "       colsample_bytree=1.0, learning_rate=0.1, max_depth=-1,\n",
      "       min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "       n_estimators=100, n_estimtors=100, n_jobs=-1, num_leaves=31,\n",
      "       objective='quantile', random_state=269, reg_alpha=0.0,\n",
      "       reg_lambda=0.0, silent=True, subsample=1.0,\n",
      "       subsample_for_bin=200000, subsample_freq=1), estimator__learning_rate=0.12387064863290859, estimator__max_depth=16, estimator__num_leaves=430, imputer__strategy=most_frequent, scaler=None, total=  49.8s\n",
      "[CV]  estimator=LGBMRegressor(alpha=0.5, boosting_type='gbdt', class_weight=None,\n",
      "       colsample_bytree=1.0, learning_rate=0.1, max_depth=-1,\n",
      "       min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "       n_estimators=100, n_estimtors=100, n_jobs=-1, num_leaves=31,\n",
      "       objective='quantile', random_state=269, reg_alpha=0.0,\n",
      "       reg_lambda=0.0, silent=True, subsample=1.0,\n",
      "       subsample_for_bin=200000, subsample_freq=1), estimator__learning_rate=0.12387064863290859, estimator__max_depth=16, estimator__num_leaves=430, imputer__strategy=most_frequent, scaler=None, total=  54.1s\n",
      "[CV]  estimator=LGBMRegressor(alpha=0.5, boosting_type='gbdt', class_weight=None,\n",
      "       colsample_bytree=1.0, learning_rate=0.1, max_depth=-1,\n",
      "       min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "       n_estimators=100, n_estimtors=100, n_jobs=-1, num_leaves=31,\n",
      "       objective='quantile', random_state=269, reg_alpha=0.0,\n",
      "       reg_lambda=0.0, silent=True, subsample=1.0,\n",
      "       subsample_for_bin=200000, subsample_freq=1), estimator__learning_rate=0.12387064863290859, estimator__max_depth=16, estimator__num_leaves=430, imputer__strategy=most_frequent, scaler=None, total=  50.7s\n",
      "[CV]  estimator=LGBMRegressor(alpha=0.5, boosting_type='gbdt', class_weight=None,\n",
      "       colsample_bytree=1.0, learning_rate=0.1, max_depth=-1,\n",
      "       min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "       n_estimators=100, n_estimtors=100, n_jobs=-1, num_leaves=31,\n",
      "       objective='quantile', random_state=269, reg_alpha=0.0,\n",
      "       reg_lambda=0.0, silent=True, subsample=1.0,\n",
      "       subsample_for_bin=200000, subsample_freq=1), estimator__learning_rate=0.12387064863290859, estimator__max_depth=16, estimator__num_leaves=430, imputer__strategy=most_frequent, scaler=None, total=  53.0s\n",
      "[CV]  estimator=LGBMRegressor(alpha=0.5, boosting_type='gbdt', class_weight=None,\n",
      "       colsample_bytree=1.0, learning_rate=0.1, max_depth=-1,\n",
      "       min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "       n_estimators=100, n_estimtors=100, n_jobs=-1, num_leaves=31,\n",
      "       objective='quantile', random_state=269, reg_alpha=0.0,\n",
      "       reg_lambda=0.0, silent=True, subsample=1.0,\n",
      "       subsample_for_bin=200000, subsample_freq=1), estimator__learning_rate=0.12387064863290859, estimator__max_depth=16, estimator__num_leaves=430, imputer__strategy=most_frequent, scaler=None, total=  51.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done  24 out of  24 | elapsed:  3.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=[(array([ 34,  35, ..., 330, 331]), array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33])), (array([  0,   1, ..., 330, 331]), array([34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50... 314, 315, 316, 317, 318, 319, 320, 321, 322, 323,\n",
       "       324, 325, 326, 327, 328, 329, 330, 331]))],\n",
       "          error_score=-9999,\n",
       "          estimator=Pipeline(memory=None,\n",
       "     steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('estimator', LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "       learning_rate=0.1, max_depth=-1, m....0, reg_lambda=0.0, silent=True, subsample=1.0,\n",
       "       subsample_for_bin=200000, subsample_freq=1))]),\n",
       "          fit_params=None, iid=True, n_iter=2, n_jobs=6,\n",
       "          param_distributions={'imputer__strategy': ['mean', 'median', 'most_frequent'], 'scaler': [None], 'estimator': [LGBMRegressor(alpha=0.5, boosting_type='gbdt', class_weight=None,\n",
       "       colsample_bytree=1.0, learning_rate=0.08749548119134132,\n",
       "       max_depth=9, min_child_samples=20, min_child_weight=...0>, 'estimator__num_leaves': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f1397cc2940>},\n",
       "          pre_dispatch='2*n_jobs', random_state=269, refit=True,\n",
       "          return_train_score=False,\n",
       "          scoring=make_scorer(quantile_loss, greater_is_better=False, quantile=0.9),\n",
       "          verbose=2)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm_90_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('scaler', None), ('estimator', LGBMRegressor(alpha=0.5, boosting_type='gbdt', class_weight=None,\n",
       "       colsample_bytree=1.0, learning_rate=0.08749548119134132,\n",
       "       max_depth=9, min_child_samples=20....0, reg_lambda=0.0, silent=True, subsample=1.0,\n",
       "       subsample_for_bin=200000, subsample_freq=1))])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm_90_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'estimator': LGBMRegressor(alpha=0.5, boosting_type='gbdt', class_weight=None,\n",
       "        colsample_bytree=1.0, learning_rate=0.08749548119134132,\n",
       "        max_depth=9, min_child_samples=20, min_child_weight=0.001,\n",
       "        min_split_gain=0.0, n_estimators=100, n_estimtors=100, n_jobs=-1,\n",
       "        num_leaves=495, objective='quantile', random_state=269,\n",
       "        reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,\n",
       "        subsample_for_bin=200000, subsample_freq=1),\n",
       " 'estimator__learning_rate': 0.08749548119134132,\n",
       " 'estimator__max_depth': 9,\n",
       " 'estimator__num_leaves': 495,\n",
       " 'imputer__strategy': 'mean',\n",
       " 'scaler': None}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm_90_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_90_cv_results = pd.DataFrame(lgbm_90_search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>param_imputer__strategy</th>\n",
       "      <th>param_estimator__max_depth</th>\n",
       "      <th>param_estimator__learning_rate</th>\n",
       "      <th>param_estimator__num_leaves</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-4.644586</td>\n",
       "      <td>1.711345</td>\n",
       "      <td>mean</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0874955</td>\n",
       "      <td>495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-4.714592</td>\n",
       "      <td>1.734041</td>\n",
       "      <td>most_frequent</td>\n",
       "      <td>16</td>\n",
       "      <td>0.123871</td>\n",
       "      <td>430</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_test_score  std_test_score param_imputer__strategy  \\\n",
       "0        -4.644586        1.711345                    mean   \n",
       "1        -4.714592        1.734041           most_frequent   \n",
       "\n",
       "  param_estimator__max_depth param_estimator__learning_rate  \\\n",
       "0                          9                      0.0874955   \n",
       "1                         16                       0.123871   \n",
       "\n",
       "  param_estimator__num_leaves  \n",
       "0                         495  \n",
       "1                         430  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm_cv_cols = [\"mean_test_score\", \"std_test_score\", \"param_imputer__strategy\",\n",
    "                \"param_estimator__max_depth\", \"param_estimator__learning_rate\",\n",
    "                \"param_estimator__num_leaves\"]\n",
    "                \n",
    "(lgbm_90_cv_results[lgbm_cv_cols].sort_values(\"mean_test_score\", ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Compare Benchmarks and Models Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and Evaluate 50th Percentile Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search space for normal quantile rergression\n",
    "imputation_strategies =  [\"mean\", \"median\", \"most_frequent\"]\n",
    "qr_50_param_space = {\n",
    "    \"imputer__strategy\": imputation_strategies,\n",
    "    \"estimator\": [SMWrapper(QuantReg, fit_kwargs={\"q\":0.5})]\n",
    "}\n",
    "\n",
    "# # search space for LGBM quantile regression\n",
    "# lgbm_50_param_space = {\n",
    "#     \"imputer__strategy\": Categorical(imputation_strategies),\n",
    "#     \"scaler\": Categorical([None]),\n",
    "#     \"estimator\": Categorical([LGBMRegressor(random_state=RANDOM_STATE,\n",
    "#                                             alpha=0.5,\n",
    "#                                             objective=\"quantile\",\n",
    "#                                             n_estimtors=100)]),\n",
    "#     \"estimator__max_depth\": Integer(1, 25),\n",
    "#     \"estimator__learning_rate\" : Real(0.01, 0.3),\n",
    "#     \"estimator__num_leaves\": Integer(10, 500)\n",
    "# }\n",
    "\n",
    "# search space for LGBM quantile regression\n",
    "lgbm_50_param_space = {\n",
    "    \"imputer__strategy\": imputation_strategies,\n",
    "    \"scaler\": [None],\n",
    "    \"estimator\": [LGBMRegressor(random_state=RANDOM_STATE,\n",
    "                                alpha=0.5,\n",
    "                                objective=\"quantile\",\n",
    "                                n_estimtors=100)],\n",
    "    \"estimator__max_depth\": sp_randint(1, 25),\n",
    "    \"estimator__learning_rate\" : sp_uniform(0.01, 0.3),\n",
    "    \"estimator__num_leaves\": sp_randint(10, 500)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate QR 50\n",
    "#### Use gridsearch for normal quantile regression since we are only searching over imputation strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 12 folds for each of 3 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/savvas/miniconda3/envs/nfl_quant/lib/python3.6/site-packages/statsmodels/regression/quantile_regression.py:193: IterationLimitWarning: Maximum number of iterations (1000) reached.\n",
      "  \") reached.\", IterationLimitWarning)\n",
      "/home/savvas/miniconda3/envs/nfl_quant/lib/python3.6/site-packages/statsmodels/regression/quantile_regression.py:193: IterationLimitWarning: Maximum number of iterations (1000) reached.\n",
      "  \") reached.\", IterationLimitWarning)\n",
      "/home/savvas/miniconda3/envs/nfl_quant/lib/python3.6/site-packages/statsmodels/regression/quantile_regression.py:193: IterationLimitWarning: Maximum number of iterations (1000) reached.\n",
      "  \") reached.\", IterationLimitWarning)\n",
      "/home/savvas/miniconda3/envs/nfl_quant/lib/python3.6/site-packages/statsmodels/regression/quantile_regression.py:193: IterationLimitWarning: Maximum number of iterations (1000) reached.\n",
      "  \") reached.\", IterationLimitWarning)\n",
      "/home/savvas/miniconda3/envs/nfl_quant/lib/python3.6/site-packages/statsmodels/regression/quantile_regression.py:193: IterationLimitWarning: Maximum number of iterations (1000) reached.\n",
      "  \") reached.\", IterationLimitWarning)\n",
      "/home/savvas/miniconda3/envs/nfl_quant/lib/python3.6/site-packages/statsmodels/regression/quantile_regression.py:193: IterationLimitWarning: Maximum number of iterations (1000) reached.\n",
      "  \") reached.\", IterationLimitWarning)\n",
      "[Parallel(n_jobs=6)]: Done  36 out of  36 | elapsed:    5.7s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=[(array([ 34,  35, ..., 330, 331]), array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33])), (array([  0,   1, ..., 330, 331]), array([34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50... 314, 315, 316, 317, 318, 319, 320, 321, 322, 323,\n",
       "       324, 325, 326, 327, 328, 329, 330, 331]))],\n",
       "       error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('estimator', LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "       learning_rate=0.1, max_depth=-1, m....0, reg_lambda=0.0, silent=True, subsample=1.0,\n",
       "       subsample_for_bin=200000, subsample_freq=1))]),\n",
       "       fit_params=None, iid=True, n_jobs=6,\n",
       "       param_grid={'imputer__strategy': ['mean', 'median', 'most_frequent'], 'estimator': [SMWrapper(fit_intercept=True, fit_kwargs={'q': 0.5},\n",
       "     model_class=<class 'statsmodels.regression.quantile_regression.QuantReg'>)]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "       scoring=make_scorer(quantile_loss, greater_is_better=False, quantile=0.5),\n",
       "       verbose=1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qr_50_grid_search = GridSearchCV(pipe, n_jobs=N_JOBS, param_grid=qr_50_param_space,\n",
    "                                 cv=cv, scoring=quant_50_scorer, verbose=1,\n",
    "                                 return_train_score=False)\n",
    "qr_50_grid_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('estimator', SMWrapper(fit_intercept=True, fit_kwargs={'q': 0.5},\n",
       "     model_class=<class 'statsmodels.regression.quantile_regression.QuantReg'>))])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qr_50_grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "qr_50_cv_results = pd.DataFrame(qr_50_grid_search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>param_imputer__strategy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-3.824141</td>\n",
       "      <td>0.893993</td>\n",
       "      <td>median</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-3.831512</td>\n",
       "      <td>0.881978</td>\n",
       "      <td>most_frequent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-3.844637</td>\n",
       "      <td>0.819394</td>\n",
       "      <td>mean</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_test_score  std_test_score param_imputer__strategy\n",
       "1        -3.824141        0.893993                  median\n",
       "2        -3.831512        0.881978           most_frequent\n",
       "0        -3.844637        0.819394                    mean"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(qr_50_cv_results[[\"mean_test_score\", \"std_test_score\", \"param_imputer__strategy\"]]\n",
    "     .sort_values(\"mean_test_score\", ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate LGBM 50\n",
    "\n",
    "### Here we will use Bayesian Optimization to tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgbm_50_search = BayesSearchCV(pipe, \n",
    "#                             lgbm_50_param_space, \n",
    "#                             cv=cv,\n",
    "#                             n_jobs=N_JOBS, \n",
    "#                             verbose=1, \n",
    "#                             error_score=-9999, \n",
    "#                             scoring=quant_50_scorer, \n",
    "#                             random_state=RANDOM_STATE,\n",
    "#                             return_train_score=False, \n",
    "#                             n_iter=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_50_search = RandomizedSearchCV(pipe, \n",
    "                                    lgbm_50_param_space, \n",
    "                                    cv=cv,\n",
    "                                    n_jobs=N_JOBS, \n",
    "                                    verbose=1, \n",
    "                                    error_score=-9999, \n",
    "                                    scoring=quant_50_scorer, \n",
    "                                    random_state=RANDOM_STATE,\n",
    "                                    return_train_score=False, \n",
    "                                    n_iter=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 12 folds for each of 2 candidates, totalling 24 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-daa39f5f554b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlgbm_50_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/nfl_quant/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    637\u001b[0m                                   error_score=self.error_score)\n\u001b[1;32m    638\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[0;32m--> 639\u001b[0;31m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nfl_quant/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nfl_quant/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    697\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nfl_quant/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nfl_quant/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nfl_quant/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nfl_quant/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lgbm_50_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_50_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_50_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_50_cv_results = pd.DataFrame(lgbm_50_search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lgbm_cv_cols = [\"mean_test_score\", \"std_test_score\", \"param_imputer__strategy\",\n",
    "                \"param_estimator__max_depth\", \"param_estimator__learning_rate\",\n",
    "                \"param_estimator__num_leaves\"]\n",
    "                \n",
    "(lgbm_50_cv_results[lgbm_cv_cols].sort_values(\"mean_test_score\", ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Compare Benchmarks and Models Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and Evaluate 10th quantile Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search space for normal quantile rergression\n",
    "imputation_strategies =  [\"mean\", \"median\", \"most_frequent\"]\n",
    "qr_10_param_space = {\n",
    "    \"imputer__strategy\": imputation_strategies,\n",
    "    \"estimator\": [SMWrapper(QuantReg, fit_kwargs={\"q\":0.1})]\n",
    "}\n",
    "\n",
    "# search space for LGBM quantile regression\n",
    "lgbm_10_param_space = {\n",
    "    \"imputer__strategy\": Categorical(imputation_strategies),\n",
    "    \"scaler\": Categorical([None]),\n",
    "    \"estimator\": Categorical([LGBMRegressor(random_state=RANDOM_STATE,\n",
    "                                            alpha=0.1,\n",
    "                                            objective=\"quantile\",\n",
    "                                            n_estimtors=100)]),\n",
    "    \"estimator__max_depth\": Integer(1, 25),\n",
    "    \"estimator__learning_rate\" : Real(0.01, 0.3),\n",
    "    \"estimator__num_leaves\": Integer(10, 500)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate QR 10\n",
    "#### Use gridsearch for normal quantile regression since we are only searching over imputation strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 12 folds for each of 3 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done  36 out of  36 | elapsed:    1.7s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=[(array([ 34,  35, ..., 330, 331]), array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33])), (array([  0,   1, ..., 330, 331]), array([34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50... 314, 315, 316, 317, 318, 319, 320, 321, 322, 323,\n",
       "       324, 325, 326, 327, 328, 329, 330, 331]))],\n",
       "       error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('estimator', LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "       learning_rate=0.1, max_depth=-1, m....0, reg_lambda=0.0, silent=True, subsample=1.0,\n",
       "       subsample_for_bin=200000, subsample_freq=1))]),\n",
       "       fit_params=None, iid=True, n_jobs=6,\n",
       "       param_grid={'imputer__strategy': ['mean', 'median', 'most_frequent'], 'estimator': [SMWrapper(fit_intercept=True, fit_kwargs={'q': 0.1},\n",
       "     model_class=<class 'statsmodels.regression.quantile_regression.QuantReg'>)]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "       scoring=make_scorer(quantile_loss, greater_is_better=False, quantile=0.1),\n",
       "       verbose=1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qr_10_grid_search = GridSearchCV(pipe, n_jobs=N_JOBS, param_grid=qr_10_param_space,\n",
    "                                 cv=cv, scoring=quant_10_scorer, verbose=1,\n",
    "                                 return_train_score=False)\n",
    "qr_10_grid_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('estimator', SMWrapper(fit_intercept=True, fit_kwargs={'q': 0.1},\n",
       "     model_class=<class 'statsmodels.regression.quantile_regression.QuantReg'>))])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qr_10_grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "qr_10_cv_results = pd.DataFrame(qr_10_grid_search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>param_imputer__strategy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.859940</td>\n",
       "      <td>0.277712</td>\n",
       "      <td>mean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.865078</td>\n",
       "      <td>0.274820</td>\n",
       "      <td>median</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.865468</td>\n",
       "      <td>0.274632</td>\n",
       "      <td>most_frequent</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_test_score  std_test_score param_imputer__strategy\n",
       "0        -0.859940        0.277712                    mean\n",
       "1        -0.865078        0.274820                  median\n",
       "2        -0.865468        0.274632           most_frequent"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(qr_10_cv_results[[\"mean_test_score\", \"std_test_score\", \"param_imputer__strategy\"]]\n",
    "     .sort_values(\"mean_test_score\", ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate LGBM 10\n",
    "\n",
    "### Here we will use Bayesian Optimization to tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_10_search = BayesSearchCV(pipe, \n",
    "                                     lgbm_10_param_space, \n",
    "                                     cv=cv,\n",
    "                                     n_jobs=N_JOBS, \n",
    "                                     verbose=1, \n",
    "                                     error_score=-9999, \n",
    "                                     scoring=quant_10_scorer, \n",
    "                                     random_state=RANDOM_STATE,\n",
    "                                     return_train_score=False, \n",
    "                                     n_iter=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 12 folds for each of 1 candidates, totalling 12 fits\n",
      "Fitting 12 folds for each of 1 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done  12 out of  12 | elapsed:  1.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 12 folds for each of 1 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done  12 out of  12 | elapsed:  1.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 12 folds for each of 1 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done  12 out of  12 | elapsed:  1.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 12 folds for each of 1 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done  12 out of  12 | elapsed:  1.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 12 folds for each of 1 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done  12 out of  12 | elapsed:  1.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 12 folds for each of 1 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done  12 out of  12 | elapsed:  1.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 12 folds for each of 1 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done  12 out of  12 | elapsed:  1.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 12 folds for each of 1 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done  12 out of  12 | elapsed:  1.3min finished\n",
      "[Parallel(n_jobs=6)]: Done  12 out of  12 | elapsed:  1.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 12 folds for each of 1 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done  12 out of  12 | elapsed:  1.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 12 folds for each of 1 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done  12 out of  12 | elapsed:  1.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 12 folds for each of 1 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done  12 out of  12 | elapsed:   20.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 12 folds for each of 1 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done  12 out of  12 | elapsed:  1.7min finished\n",
      "/home/savvas/miniconda3/envs/nfl_quant/lib/python3.6/site-packages/skopt/optimizer/optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 12 folds for each of 1 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done  12 out of  12 | elapsed:   19.9s finished\n",
      "/home/savvas/miniconda3/envs/nfl_quant/lib/python3.6/site-packages/skopt/optimizer/optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 12 folds for each of 1 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done  12 out of  12 | elapsed:   20.3s finished\n",
      "/home/savvas/miniconda3/envs/nfl_quant/lib/python3.6/site-packages/skopt/optimizer/optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 12 folds for each of 1 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done  12 out of  12 | elapsed:   19.8s finished\n",
      "/home/savvas/miniconda3/envs/nfl_quant/lib/python3.6/site-packages/skopt/optimizer/optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 12 folds for each of 1 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done  12 out of  12 | elapsed:   20.1s finished\n",
      "/home/savvas/miniconda3/envs/nfl_quant/lib/python3.6/site-packages/skopt/optimizer/optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 12 folds for each of 1 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done  12 out of  12 | elapsed:   20.4s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BayesSearchCV(cv=[(array([ 34,  35, ..., 330, 331]), array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33])), (array([  0,   1, ..., 330, 331]), array([34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50... 314, 315, 316, 317, 318, 319, 320, 321, 322, 323,\n",
       "       324, 325, 326, 327, 328, 329, 330, 331]))],\n",
       "       error_score=-9999,\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('estimator', LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "       learning_rate=0.1, max_depth=-1, m....0, reg_lambda=0.0, silent=True, subsample=1.0,\n",
       "       subsample_for_bin=200000, subsample_freq=1))]),\n",
       "       fit_params=None, iid=True, n_iter=18, n_jobs=6, n_points=1,\n",
       "       optimizer_kwargs=None, pre_dispatch='2*n_jobs', random_state=269,\n",
       "       refit=True, return_train_score=False,\n",
       "       scoring=make_scorer(quantile_loss, greater_is_better=False, quantile=0.1),\n",
       "       search_spaces={'imputer__strategy': Categorical(categories=('mean', 'median', 'most_frequent'), prior=None), 'scaler': Categorical(categories=(None,), prior=None), 'estimator': Categorical(categories=(LGBMRegressor(alpha=0.1, boosting_type='gbdt', class_weight=None,\n",
       "       colsample_bytree=1.0, lear...igh=0.3, prior='uniform', transform='identity'), 'estimator__num_leaves': Integer(low=10, high=500)},\n",
       "       verbose=1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm_10_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('scaler', None), ('estimator', LGBMRegressor(alpha=0.1, boosting_type='gbdt', class_weight=None,\n",
       "       colsample_bytree=1.0, learning_rate=0.01, max_depth=1,\n",
       "       min_child_samples=20, min_child_w...    reg_lambda=0.0, silent=True, subsample=1.0,\n",
       "       subsample_for_bin=200000, subsample_freq=1))])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm_10_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'estimator': LGBMRegressor(alpha=0.1, boosting_type='gbdt', class_weight=None,\n",
       "        colsample_bytree=1.0, learning_rate=0.01, max_depth=1,\n",
       "        min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
       "        n_estimators=100, n_estimtors=100, n_jobs=-1, num_leaves=10,\n",
       "        objective='quantile', random_state=269, reg_alpha=0.0,\n",
       "        reg_lambda=0.0, silent=True, subsample=1.0,\n",
       "        subsample_for_bin=200000, subsample_freq=1),\n",
       " 'estimator__learning_rate': 0.01,\n",
       " 'estimator__max_depth': 1,\n",
       " 'estimator__num_leaves': 10,\n",
       " 'imputer__strategy': 'median',\n",
       " 'scaler': None}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm_10_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_10_cv_results = pd.DataFrame(lgbm_10_search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>param_imputer__strategy</th>\n",
       "      <th>param_estimator__max_depth</th>\n",
       "      <th>param_estimator__learning_rate</th>\n",
       "      <th>param_estimator__num_leaves</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.864790</td>\n",
       "      <td>0.275990</td>\n",
       "      <td>median</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.864790</td>\n",
       "      <td>0.275990</td>\n",
       "      <td>median</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.864790</td>\n",
       "      <td>0.275990</td>\n",
       "      <td>median</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.864790</td>\n",
       "      <td>0.275990</td>\n",
       "      <td>median</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.864790</td>\n",
       "      <td>0.275990</td>\n",
       "      <td>median</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.864790</td>\n",
       "      <td>0.275990</td>\n",
       "      <td>median</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.877791</td>\n",
       "      <td>0.261756</td>\n",
       "      <td>median</td>\n",
       "      <td>25</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.904367</td>\n",
       "      <td>0.273335</td>\n",
       "      <td>mean</td>\n",
       "      <td>12</td>\n",
       "      <td>0.018729</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.930864</td>\n",
       "      <td>0.290538</td>\n",
       "      <td>mean</td>\n",
       "      <td>12</td>\n",
       "      <td>0.031573</td>\n",
       "      <td>215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.932517</td>\n",
       "      <td>0.280192</td>\n",
       "      <td>median</td>\n",
       "      <td>11</td>\n",
       "      <td>0.033568</td>\n",
       "      <td>240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.938523</td>\n",
       "      <td>0.259861</td>\n",
       "      <td>mean</td>\n",
       "      <td>24</td>\n",
       "      <td>0.051682</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.966993</td>\n",
       "      <td>0.310975</td>\n",
       "      <td>most_frequent</td>\n",
       "      <td>10</td>\n",
       "      <td>0.098298</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.978627</td>\n",
       "      <td>0.271378</td>\n",
       "      <td>mean</td>\n",
       "      <td>6</td>\n",
       "      <td>0.085606</td>\n",
       "      <td>334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.981396</td>\n",
       "      <td>0.320374</td>\n",
       "      <td>most_frequent</td>\n",
       "      <td>8</td>\n",
       "      <td>0.094628</td>\n",
       "      <td>479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.989517</td>\n",
       "      <td>0.317267</td>\n",
       "      <td>most_frequent</td>\n",
       "      <td>7</td>\n",
       "      <td>0.139482</td>\n",
       "      <td>308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.990938</td>\n",
       "      <td>0.266751</td>\n",
       "      <td>mean</td>\n",
       "      <td>4</td>\n",
       "      <td>0.208841</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.001086</td>\n",
       "      <td>0.297218</td>\n",
       "      <td>mean</td>\n",
       "      <td>12</td>\n",
       "      <td>0.148320</td>\n",
       "      <td>491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.016921</td>\n",
       "      <td>0.304040</td>\n",
       "      <td>mean</td>\n",
       "      <td>12</td>\n",
       "      <td>0.161616</td>\n",
       "      <td>392</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_test_score  std_test_score param_imputer__strategy  \\\n",
       "17        -0.864790        0.275990                  median   \n",
       "11        -0.864790        0.275990                  median   \n",
       "16        -0.864790        0.275990                  median   \n",
       "15        -0.864790        0.275990                  median   \n",
       "14        -0.864790        0.275990                  median   \n",
       "13        -0.864790        0.275990                  median   \n",
       "12        -0.877791        0.261756                  median   \n",
       "9         -0.904367        0.273335                    mean   \n",
       "6         -0.930864        0.290538                    mean   \n",
       "10        -0.932517        0.280192                  median   \n",
       "8         -0.938523        0.259861                    mean   \n",
       "5         -0.966993        0.310975           most_frequent   \n",
       "4         -0.978627        0.271378                    mean   \n",
       "0         -0.981396        0.320374           most_frequent   \n",
       "3         -0.989517        0.317267           most_frequent   \n",
       "7         -0.990938        0.266751                    mean   \n",
       "2         -1.001086        0.297218                    mean   \n",
       "1         -1.016921        0.304040                    mean   \n",
       "\n",
       "    param_estimator__max_depth  param_estimator__learning_rate  \\\n",
       "17                           1                        0.010000   \n",
       "11                           1                        0.010000   \n",
       "16                           1                        0.010000   \n",
       "15                           1                        0.010000   \n",
       "14                           1                        0.010000   \n",
       "13                           1                        0.010000   \n",
       "12                          25                        0.010000   \n",
       "9                           12                        0.018729   \n",
       "6                           12                        0.031573   \n",
       "10                          11                        0.033568   \n",
       "8                           24                        0.051682   \n",
       "5                           10                        0.098298   \n",
       "4                            6                        0.085606   \n",
       "0                            8                        0.094628   \n",
       "3                            7                        0.139482   \n",
       "7                            4                        0.208841   \n",
       "2                           12                        0.148320   \n",
       "1                           12                        0.161616   \n",
       "\n",
       "    param_estimator__num_leaves  \n",
       "17                           10  \n",
       "11                           10  \n",
       "16                           10  \n",
       "15                           10  \n",
       "14                           10  \n",
       "13                           10  \n",
       "12                           10  \n",
       "9                            86  \n",
       "6                           215  \n",
       "10                          240  \n",
       "8                           192  \n",
       "5                            51  \n",
       "4                           334  \n",
       "0                           479  \n",
       "3                           308  \n",
       "7                           113  \n",
       "2                           491  \n",
       "1                           392  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm_cv_cols = [\"mean_test_score\", \"std_test_score\", \"param_imputer__strategy\",\n",
    "                \"param_estimator__max_depth\", \"param_estimator__learning_rate\",\n",
    "                \"param_estimator__num_leaves\"]\n",
    "                \n",
    "(lgbm_10_cv_results[lgbm_cv_cols].sort_values(\"mean_test_score\", ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Compare Benchmarks and Models Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
